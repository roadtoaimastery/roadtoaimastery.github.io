---
layout: post
title: 1-hour Crash-Course in Linear Algebra
math: true
---
<div style="text-align: center; margin-bottom: 2rem;">
  <img src="/assets/img/posts/linear_algebra.jpeg" 
       alt="Overview of Linear Algebra" 
       style="width: 100%; object-fit: cover; border-radius: 15px; rgba(0,0,0,0.2);" />
  <p style="margin-top: 1rem; font-style: italic; color: #666; font-size: 0.9rem;">
    Overview of Linear Algebra
  </p>
</div>


This 1-hour crash course is based on the prompts from [Learning Linear Algebra in One Hour](/posts/learning-linear-algebra-in-one-hour-using-ai/). I have added my knowledge into AI to make more complete course!

## What's Linear Algebra?

- Linear algebra is the branch of mathematics concerned with vector spaces, linear transformations, and systems of linear equations. 
- At its core, it studies how vectors and matrices can represent and manipulate data, often revealing structure through operations like dot products, eigenvalues, and matrix factorizations.

### Applications:
1. **Computer Graphics**  
   - Creates and manipulates 3D scenes using matrices and vectors.  
   - Matrices handle rotation, scaling, and translation.  
   - Vectors represent positions and directions in space.

2. **Machine Learning**  
   - Models learn from data organized in matrices.  
   - Involves operations like gradient descent and linear mappings.  
   - Uses concepts like dot product, SVD, and eigendecomposition.

3. **Quantum Mechanics**  
   - Describes physical systems using vector spaces (Hilbert spaces).  
   - Measurements are eigenvalues; quantum states are eigenvectors.  
   - Operators are linear transformations on these spaces.

---

## Cheat Sheet

### ðŸ“Œ Vectors & Vector Spaces
- **Vector**:
  $$\mathbf{a} \in \mathbb{R}^n$$
- **Dot Product**:  
  $$\mathbf{a} \cdot \mathbf{b} = \sum a_i b_i = \|\mathbf{a}\|\|\mathbf{b}\|\cos\theta$$

---

### ðŸ“Œ Matrices & Linear Maps
- **Matrix multiplication**:  
  $$C = AB, \quad C_{ij} = \sum_k A_{ik} B_{kj}$$

- **2D Transformations**: rotation, scaling, translation (homogeneous coords)

---

### ðŸ“Œ Solving $$A\vec{x} = \vec{b}$$
- **Inverse**:  
  $$\vec{x} = A^{-1} \vec{b}$$

- **Pseudoinverse** (for ML):  
  $$A^+ = (A^T A)^{-1} A^T$$

---

### ðŸ“Œ Matrix Properties
- **Determinant**: $$\det(A) \neq 0$$ â‡’ invertible  
- **Trace**: $$\text{tr}(A) = \sum A_{ii}$$  
- **Rank**: number of linearly independent rows/columns

---

### ðŸ“Œ Eigenvalues & Eigenvectors
- Equation:  
  $$A\vec{v} = \lambda \vec{v}$$
- Found by solving $$\det(A - \lambda I) = 0$$

---

### ðŸ“Œ Orthonormality
- **Projection**:  
  $$\text{proj}_{\vec{a}} \vec{b} = \frac{\vec{a} \cdot \vec{b}}{\|\vec{a}\|^2} \vec{a}$$

---

### ðŸ“Œ Singular Value Decomposition (SVD)
- Decomposition:  
  $$A = U \Sigma V^T$$
- Used in compression, ML, quantum state structure

---

### ðŸ”š Applications Table

| Concept     | Graphics | Machine Learning    | Quantum Mechanics       |
| ----------- | -------- | ------------------- | ----------------------- |
| Transforms  | Matrices | Feature spaces      | State evolution         |
| Dot Product | Lighting | Similarity, kernels | Inner product of states |
| Eigenvalues | NA       | PCA, optimization   | Observable measurement  |
| SVD         | NA       | Data compression    | Quantum state structure |

---

## Application Breakdown â€“ Tutor for a 10-Year-Old

---

### Application 1: Transforms (Graphics)

**Goal**: Move, spin, and scale objects using **vectors and matrices**.

Steps:
1. Vectors = positions (e.g., where a LEGO figure is).
2. Matrices = how to rotate, scale, or move objects.
3. Multiply Matrix Ã— Vector to get a new position.
4. Use special matrices for each action (rotate, scale, translate).
5. Combine multiple actions by multiplying matrices.
6. Games use this every frame to update object positions on screen.

---

### Application 2: Dot Product (Lighting in Graphics)

**Goal**: Figure out **how bright** something looks when a light hits it.

Steps:
1. Represent light direction and surface direction as vectors.
2. Use the dot product to find how aligned they are.
3. Dot product formula:  
   \[
   \vec{a} \cdot \vec{b} = \|\vec{a}\|\|\vec{b}\|\cos\theta
   \]
4. Big dot product = bright light; small = dim; negative = no light.
5. Used in every lighting and shading system in games.

---

### Application 3: Eigenvalues (Quantum Physics)

**Goal**: Find directions that don't rotate under transformation â€” they just stretch/shrink.

Steps:
1. A matrix transforms vectors (stretches/rotates them).
2. Eigenvectors are special vectors that don't rotate â€” just scale.
3. Equation:  
   \[
   A\vec{v} = \lambda \vec{v}
   \]
4. Î» = eigenvalue (how much it stretches/shrinks).
5. In quantum mechanics: eigenvalues = measurement outcomes, eigenvectors = possible states.

---

### Application 4: SVD (Data Compression & ML)

**Goal**: Break a big messy matrix into neat parts.

Steps:
1. Matrix = data (like a photo or table).
2. SVD splits it into 3 parts:  
   \[
   A = U \Sigma V^T
   \]
   - U = direction info  
   - Î£ = importance (singular values)  
   - Váµ— = how things are arranged
3. Keep only the big values in Î£ to compress or simplify the data.
4. Used in ML for pattern recognition, compression, and reducing noise.

---

